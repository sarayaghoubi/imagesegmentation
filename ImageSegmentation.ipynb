{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sarayaghoubi/sara/blob/master/ImageSegmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cM9Yg0GCkxaR"
      },
      "source": [
        "Mount the drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cBKApn4fNnuZ",
        "outputId": "c41406ef-f5b1-4a79-ad7a-87e848c30672"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oa6JPXW0k1B9"
      },
      "source": [
        "download the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vO3uGijFsDAk"
      },
      "outputs": [],
      "source": [
        "# !wget https://zenodo.org/record/5706578/files/Train.zip -P /content/drive/MyDrive/data/Train\n",
        "# !unzip /content/drive/MyDrive/data/Train/Train.zip -d /content/drive/MyDrive/data/Train/\n",
        "\n",
        "#!wget https://zenodo.org/record/5706578/files/Val.zip -P /content/drive/MyDrive/data/Val\n",
        "#!unzip /content/drive/MyDrive/data/Val/Val.zip -d /content/drive/MyDrive/data/Val/\n",
        "\n",
        "# !wget https://zenodo.org/record/5706578/files/Test.zip -P /content/drive/MyDrive/data/Test\n",
        "# !unzip /content/drive/MyDrive/data/Test2/Test.zip -d /content/drive/MyDrive/data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "860JjZRbk4ZQ"
      },
      "source": [
        "install libraries, dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PgVu_jjj5nrU"
      },
      "outputs": [],
      "source": [
        "!pip install -U torch==1.9.0+cu111 torchvision==0.10.0+cu111 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cu111/torch1.9.0/index.html\n",
        "!git clone https://github.com/open-mmlab/mmsegmentation.git\n",
        "%cd mmsegmentation\n",
        "!pip install -e .\n",
        "import mmseg\n",
        "address = '/content/drive'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCUE3wgnk7w1"
      },
      "source": [
        "to handle the data, create the class, resize the pictures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XSaQOSJ8iXt0"
      },
      "outputs": [],
      "source": [
        "# Last update was on  1 May\n",
        "import os.path as osp\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import os\n",
        "import cv2\n",
        "import mmcv\n",
        "from mmseg.datasets.builder import DATASETS\n",
        "from mmseg.datasets.custom import CustomDataset\n",
        "\n",
        "\n",
        "def annotate(img, label):\n",
        "    \"\"\"\n",
        "    The reason why this method was built this method was to create an annotation map\n",
        "    if the output was meant to be in GrayScale this function must be called\n",
        "    :param img: the image\n",
        "    :param label: the colors was used in the segmentation maps\n",
        "    :return: 2D array that has mapped the colors to the classes' id\n",
        "    \"\"\"\n",
        "    size = img.shape\n",
        "    annotation = np.zeros(size[0:2])\n",
        "    i = 0\n",
        "    for color in label:\n",
        "        z = np.where(img == color)[0:2]\n",
        "        annotation[z] = i\n",
        "        i += 1\n",
        "    return annotation\n",
        "\n",
        "\n",
        "class Data:\n",
        "    \"\"\"\n",
        "    So this class was mainly intended to deal with our custom dataset, since it had originally\n",
        "    images with different size, the output needed to be reconstructed\n",
        "    Arguments:\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.std = None\n",
        "        self.mean = None\n",
        "        self.n_classes = None\n",
        "        self.classes = []\n",
        "        self.palette = None\n",
        "        self.data_root = root\n",
        "        self.test_root = data_spc['test_address']\n",
        "        self.validation_root, self.val_annotation = data_spc['validation_address']\n",
        "        self.img_postfix, self.img_dir = data_spc['train_image']\n",
        "        self.ann_postfix, self.ann_dir = data_spc['train_annotation']\n",
        "        self.data_type = 'AerialDataset'\n",
        "\n",
        "    def process_data(self, new_folder, size, require_resize):\n",
        "        \"\"\"\n",
        "        first it will resize the image to make sure all the images are in the same shape\n",
        "        then find the palette\n",
        "        secondly, find the mean and the std of input images\n",
        "        Arguments:\n",
        "            new_folder : the folder that all the new images (after resizing) will be saved\n",
        "            size : must be equal to the backbone-input image\n",
        "            require_resize : boolean; if the images were already resized and prepared\n",
        "        ** please be noticed that the\n",
        "\n",
        "        \"\"\"\n",
        "        if require_resize:\n",
        "            self.resize(new_folder, size)\n",
        "            self.img_dir = new_folder\n",
        "        self.palette, self.classes = self.spc()\n",
        "        self.mean, self.std = self.statistics()\n",
        "        self.n_classes = len(self.palette)\n",
        "\n",
        "    @staticmethod\n",
        "    def spc():\n",
        "        color =[[255, 255, 255], [255, 0, 0], [255, 255, 0], [0, 0, 255],\n",
        "                 [159, 129, 183], [0, 255, 0], [255, 195, 128]]\n",
        "        classes = ('background', 'building', 'road', 'water', 'barren', 'forest',\n",
        "                   'agricultural')\n",
        "        return color, classes\n",
        "\n",
        "    @staticmethod\n",
        "    def statistics():  # as each image we have has different shape\n",
        "        \"\"\"\n",
        "\n",
        "        :return:  the mean and standard deviation of the images (these will be required for the config file\n",
        "        \"\"\"\n",
        "        return [123.675, 116.28, 103.53], [58.395, 57.12, 57.375]\n",
        "\n",
        "    def resize(self, new_directory, size):\n",
        "        label, classes = self.spc()\n",
        "        for file in os.listdir(osp.join(self.data_root, self.img_dir)):\n",
        "            address = osp.join(self.data_root, self.img_dir, file)\n",
        "            img = cv2.resize(np.array(Image.open(address).convert('RGB'), dtype=np.uint8), size,\n",
        "                             interpolation=cv2.INTER_NEAREST)\n",
        "            if file.endswith('jpg'):\n",
        "                img = Image.fromarray(img).convert('RGB')\n",
        "            if file.endswith('png'):\n",
        "                img = annotate(img, label)\n",
        "                img = Image.fromarray(img).convert('P')\n",
        "            img.save(osp.join(self.data_root, new_directory, file))\n",
        "\n",
        "    def reconstruct(self, data_config, ignore):\n",
        "        self.process_data(data_config['new_directory'], data_config['size'], data_config['resize'])\n",
        "        split_dir = 'splits'\n",
        "        mmcv.mkdir_or_exist(osp.join(self.data_root, split_dir))\n",
        "        write_file = {\n",
        "            'train': self.img_dir,\n",
        "            'test': self.test_root,\n",
        "            'val': self.validation_root\n",
        "        }\n",
        "        for file in write_file:\n",
        "            filename_list = [osp.splitext(filename)[0] for filename in mmcv.scandir(\n",
        "                osp.join(self.data_root, write_file[file]), suffix='.png')]\n",
        "            with open(osp.join(self.data_root, split_dir, f'{file}.txt'), 'w') as f:\n",
        "                f.writelines(line + '\\n' for line in filename_list)\n",
        "        classes = self.classes\n",
        "        palette = self.palette\n",
        "        if not ignore:\n",
        "            @DATASETS.register_module()\n",
        "            class AerialDataset(CustomDataset):\n",
        "                CLASSES = classes\n",
        "                PALETTE = palette\n",
        "\n",
        "                def __init__(self, split, **kwargs):\n",
        "                    super().__init__(img_suffix='.png', seg_map_suffix='.png',\n",
        "                                     split=split, **kwargs)\n",
        "                    assert osp.exists(self.img_dir) and self.split is not None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LaAPCSeklGu8"
      },
      "source": [
        "Config file\n",
        "includes address of dataset or checkpoint files, learning parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lq552a48efna"
      },
      "outputs": [],
      "source": [
        "import os.path as pt\n",
        "\n",
        "\n",
        "root = '/content'\n",
        "mydrive = f'{root}/drive/MyDrive'\n",
        "output_address = f'{mydrive}/dlv3-res'\n",
        "img = 'images_png'\n",
        "ann = 'masks_png'\n",
        "train = 'Train'\n",
        "test = 'Test'\n",
        "saved_checkpoints_deeplabv3plus = f'{mydrive}/deeplabv3plus.pth'\n",
        "saved_checkpoints_vit = f'{mydrive}/Vit.pth'\n",
        "\n",
        "checkpoint_files = {\n",
        "    'Vit' : saved_checkpoints_vit,\n",
        "    'dlv3' : saved_checkpoints_deeplabv3plus\n",
        "\n",
        "}\n",
        "data_subfolders = f'{mydrive}/data'\n",
        "branch = 'Rural'\n",
        "data_spc = {\n",
        "    'size': (1024, 1024),\n",
        "    'resize': False,\n",
        "    'train_annotation': ('png', pt.join(data_subfolders, train,branch, ann)),\n",
        "    'train_image': ('png', pt.join(data_subfolders, train,branch, img)),\n",
        "    'new_directory': '',\n",
        "    'test_address': (pt.join(data_subfolders, test,branch, img)),\n",
        "    'validation_address': (pt.join(data_subfolders, train,branch, img), pt.join(data_subfolders, train,branch, ann))\n",
        "}\n",
        "\n",
        "\n",
        "#root = '/home/aminre/Optics/optics-segmentation/dataset/splited'\n",
        "#img = 'images_png'\n",
        "#ann = 'masks_png'\n",
        "#train = 'Train'\n",
        "#test = 'Test'\n",
        "#validation = 'Val'\n",
        "#rt = '/home/aminre/Optics/optics-segmentation/dataset/splited'\n",
        "config_files = {\n",
        "    'Vit': '/content/mmsegmentation/configs/vit/upernet_vit-b16_mln_512x512_160k_ade20k.py',\n",
        "    'Swin': '/content/mmsegmentation/configs/swin/upernet_swin_base_patch4_window7_512x512_160k_ade20k_pretrain_224x224_22K.py',\n",
        "    'dlv3': '/content/mmsegmentation/configs/deeplabv3plus/deeplabv3plus_r18b-d8_512x1024_80k_cityscapes.py'\n",
        "}\n",
        "\n",
        "color =[[0,0,0],[255, 255, 255], [255, 0, 0], [255, 255, 0], [0, 0, 255],\n",
        "                 [159, 129, 183], [0, 255, 0], [255, 195, 128]]\n",
        "classes = ('null','background', 'building', 'road', 'water', 'barren', 'forest',\n",
        "                   'agricultural')\n",
        "train_spc = {\n",
        "    'scale': (1024, 1024),\n",
        "    'crop_size':(512,512),# (128, 128),\n",
        "    'batch': 6,\n",
        "    'max': 500000,  # it only accepts either max_epochs-> for epoch based or max_iter for iter based\n",
        "    'log_int': 1000,\n",
        "    'eval': 1000,\n",
        "    'checkpoint': 1000,\n",
        "    'lr_rate' : 0.001,# initital : 300 epoch  with 500 epoch with \n",
        "    'work_directory': '',\n",
        "    'train_type' : 'EpochBasedRunner',\n",
        "    'pretrain':saved_checkpoints_deeplabv3plus,#pt.join(root, 'models', 'VitFullCrossEntropy/epoch_210.pth'),  # if you want to start\n",
        "    # training the model from scratch set it to None\n",
        "    'lr_mul':10,# 10,  # if you want to train your model's decoder with a  higher learning rate\n",
        "    'loss_a': dict(\n",
        "            type='DiceLoss', use_sigmoid=False, loss_weight=0.5),\n",
        "    'loss_d': dict(\n",
        "            type='DiceLoss', use_sigmoid=False, loss_weight=0.5),# default loss is cross entropy, other options are: 1- 'DiceLoss' 2- 'FocalLoss' 3- LovaszLoss\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5c7V9TGJf5K"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CwPYBcXmpaqF"
      },
      "outputs": [],
      "source": [
        "from mmseg.apis import set_random_seed\n",
        "from mmcv import Config\n",
        "import mmcv\n",
        "from mmseg.datasets import build_dataset\n",
        "from mmseg.models import build_segmentor\n",
        "from mmseg.apis import train_segmentor\n",
        "from mmseg.apis import inference_segmentor,show_result_pyplot\n",
        "import matplotlib.patches as m_patches\n",
        "import os.path as osp\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import os\n",
        "import torch.distributed as dist\n",
        "from mmcv.runner import (get_dist_info, init_dist, load_checkpoint,\n",
        "                         wrap_fp16_model)\n",
        "from mmcv.cnn.utils import revert_sync_batchnorm\n",
        "from mmseg.apis import init_segmentor\n",
        "\n",
        "\n",
        "class MMSEGAerialAnalysis:\n",
        "    def __init__(self, data_config):\n",
        "        self.cfg = None\n",
        "        self.model = None\n",
        "        self.data = Data()\n",
        "\n",
        "    def init_data(self, *args):\n",
        "        self.data.reconstruct(*args)\n",
        "\n",
        "    def prepare_config(self, config_dir, train):\n",
        "        cfg = Config.fromfile(config_dir)\n",
        "        # Since we use ony one GPU, BN is used instead of SyncBN\n",
        "        cfg.model.decode_head.num_classes = self.data.n_classes\n",
        "        cfg.model.auxiliary_head.num_classes = self.data.n_classes\n",
        "        cfg.norm_cfg = dict(type='BN')\n",
        "        cfg.model.decode_head.norm_cfg = dict(type='BN')\n",
        "        cfg.model.auxiliary_head.norm_cfg = dict(type='BN')\n",
        "\n",
        "        cfg.dataset_type = self.data.data_type\n",
        "        cfg.data_root = self.data.data_root\n",
        "        cfg.img_norm_cfg = dict(\n",
        "            mean=self.data.mean, std=self.data.std)\n",
        "        cfg.crop_size = train['crop_size']\n",
        "        cfg.train_pipeline = [\n",
        "            dict(type='LoadImageFromFile'),\n",
        "            dict(type='LoadAnnotations', reduce_zero_label=True),\n",
        "            dict(type='Resize', img_scale=train['scale'], ratio_range=(0.5, 2.0)),\n",
        "            dict(type='RandomCrop', crop_size=cfg.crop_size, cat_max_ratio=0.75),\n",
        "            dict(type='RandomFlip', flip_ratio=0.5),\n",
        "            dict(type='PhotoMetricDistortion'),\n",
        "            dict(type='Normalize', **cfg.img_norm_cfg),\n",
        "            dict(type='Pad', size=cfg.crop_size, pad_val=0, seg_pad_val=255),\n",
        "            dict(type='DefaultFormatBundle'),\n",
        "            dict(type='Collect', keys=['img', 'gt_semantic_seg']),\n",
        "        ]\n",
        "        cfg.test_pipeline = [\n",
        "            dict(type='LoadImageFromFile'),\n",
        "            dict(\n",
        "                type='MultiScaleFlipAug',\n",
        "                img_scale=train['scale'],\n",
        "                flip=False,\n",
        "                transforms=[\n",
        "                    dict(type='Resize', keep_ratio=True),\n",
        "                    dict(type='RandomFlip'),\n",
        "                    dict(type='Normalize', **cfg.img_norm_cfg),\n",
        "                    dict(type='ImageToTensor', keys=['img']),\n",
        "                    dict(type='Collect', keys=['img']),\n",
        "                ])\n",
        "        ]\n",
        "        cfg.data.train.type = cfg.dataset_type\n",
        "        cfg.data.train.data_root = cfg.data_root\n",
        "        cfg.data.train.img_dir = self.data.img_dir\n",
        "        cfg.data.train.ann_dir = self.data.ann_dir\n",
        "        cfg.data.train.pipeline = cfg.train_pipeline\n",
        "        cfg.data.train.split = 'splits/train.txt'\n",
        "\n",
        "        cfg.data.val.type = cfg.dataset_type\n",
        "        cfg.data.val.data_root = cfg.data_root\n",
        "        cfg.data.val.img_dir = self.data.validation_root\n",
        "        cfg.data.val.ann_dir = self.data.val_annotation\n",
        "        cfg.data.val.pipeline = cfg.test_pipeline\n",
        "        cfg.data.val.split = 'splits/val.txt'\n",
        "\n",
        "        cfg.data.test.type = cfg.dataset_type\n",
        "        cfg.data.test.data_root = cfg.data_root\n",
        "        cfg.data.test.img_dir = self.data.validation_root\n",
        "        cfg.data.test.ann_dir = self.data.val_annotation\n",
        "        cfg.data.test.pipeline = cfg.test_pipeline\n",
        "        cfg.data.test.split = 'splits/val.txt'\n",
        "        cfg.load_from = None\n",
        "        # Set up working dir to save files and logs.\n",
        "\n",
        "        cfg.model.decode_head.loss_decode = train['loss_d']\n",
        "        cfg.model.auxiliary_head.loss_decode = train['loss_a'] # ,dict(type='DiceLoss', loss_name='loss_dice', loss_weight=1.5)\n",
        "        cfg.runner = dict(type=train['train_type'], max_epochs=train['max'])\n",
        "        cfg.log_config.interval = train['log_int']\n",
        "        cfg.evaluation.interval = train['eval']\n",
        "        cfg.checkpoint_config.interval = train['checkpoint']\n",
        "        cfg.optimizer.lr = train['lr_rate']\n",
        "        cfg.optimizer.paramwise_cfg = dict(\n",
        "            custom_keys={\n",
        "                'head': dict(lr_mult=train['lr_mul'])})\n",
        "        cfg.work_dir = train['work_directory']\n",
        "        cfg.data.workers_per_gpu = 1\n",
        "        cfg.data.samples_per_gpu = train['batch']\n",
        "        # Set seed to facilitate reproducing the result\n",
        "        cfg.seed = 0\n",
        "        set_random_seed(0, deterministic=False)\n",
        "        cfg.gpu_ids = [0]\n",
        "        if train['pretrain'] is not None:\n",
        "            cfg.load_from = train['pretrain']\n",
        "        else:\n",
        "            cfg.model.pretrained = None\n",
        "        self.cfg = cfg\n",
        "\n",
        "    def init_model(self):\n",
        "        self.model = build_segmentor(\n",
        "            self.cfg.model, train_cfg=self.cfg.get('train_cfg'), test_cfg=self.cfg.get('test_cfg'))\n",
        "        self.model.CLASSES = self.data.n_classes\n",
        "        self.model.PALETTE = self.data.palette\n",
        "\n",
        "    def train_model(self, init):\n",
        "        if init:\n",
        "            dist.init_process_group(backend='nccl', init_method='tcp://localhost:23456', rank=0, world_size=1)\n",
        "        datasets = [build_dataset(self.cfg.data.train)]\n",
        "        mmcv.mkdir_or_exist(osp.abspath(self.cfg.work_dir))\n",
        "        train_segmentor(self.model, datasets, self.cfg, distributed=False, validate=True,\n",
        "                        meta=dict())\n",
        "\n",
        "    def test_model(self, img_direction, checkpoint, cfg, classes, pallete,model_name):\n",
        "        cfg = Config.fromfile(cfg)\n",
        "        cfg.model.train_cfg = None\n",
        "        cfg.model.decode_head.num_classes = len(classes)\n",
        "        cfg.model.auxiliary_head.num_classes = len(classes)\n",
        "        model = build_segmentor(\n",
        "            cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
        "        model.CLASSES = classes\n",
        "        model.PALETTE = pallete\n",
        "        model.pretrained = None\n",
        "        checkpoint = load_checkpoint(model, checkpoint, map_location='cpu')\n",
        "        model = revert_sync_batchnorm(model)\n",
        "        model.cfg = cfg\n",
        "        model.to('cpu')\n",
        "        model.eval()\n",
        "        for file in os.listdir(img_direction):\n",
        "            image = mmcv.imread(os.path.join(img_direction, file))\n",
        "            result = inference_segmentor(model, image)\n",
        "            # print(result)\n",
        "            self.show_result(image, result, pallete, classes,os.path.join(mydrive,'result',model_name,file))\n",
        "            \n",
        "    def color_img(self,seg,color):\n",
        "      for label, colo in enumerate(color):\n",
        "        color_seg[seg == label, :] = colo\n",
        "      return color_seg\n",
        "    def show_result(self,img, res, colors, labels,path):\n",
        "        res = self.color_img(res[0],colors)\n",
        "        fig = plt.figure(figsize=(16,16))\n",
        "        fig.add_subplot(1, 2, 1)\n",
        "        plt.imshow(res)\n",
        "        fig.add_subplot(1, 2, 2)\n",
        "        plt.imshow(img)\n",
        "        patches = [m_patches.Patch(color=np.array(colors[i])/ 256.,\n",
        "                                   label=labels[i]) for i in range(len(colors))]\n",
        "        # put those patched as legend-handles into the legend\n",
        "        plt.legend(handles=patches, bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.,\n",
        "                   fontsize='large')\n",
        "        plt.savefig(path)\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "color =[[0,0,0],[255, 255, 255], [255, 0, 0], [255, 255, 0], [0, 0, 255],\n",
        "                 [159, 129, 183], [0, 255, 0], [255, 195, 128]]\n",
        "# for i in range(len(color)):\n",
        "#   print(color)\n",
        "#   c = color[i]\n",
        "#   print(c)\n",
        "#   c = c[::-1]\n",
        "#   print(c)"
      ],
      "metadata": {
        "id": "QEW65davPpBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "id": "ii_fa5yKptsB"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "import gc\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    dt = [(pt.join(data_subfolders, test,'Rural', img)),(pt.join(data_subfolders, test,'Urban', img))]\n",
        "    gc.collect()\n",
        "    ignore_define = True\n",
        "    model = ['Vit','dlv3']\n",
        "    initial_process = False# if model=='dlv3' else True\n",
        "    for m in model:\n",
        "      for d in dt:\n",
        "        train_spc['work_directory'] = output_address\n",
        "        Agent = MMSEGAerialAnalysis(data_spc)\n",
        "        Agent.init_data(data_spc, ignore_define)\n",
        "        Agent.prepare_config(config_files[m], train_spc)\n",
        "        Agent.init_model()\n",
        "        # Agent.train_model(initial_process)\n",
        "        ignore_define = True\n",
        "        Agent.test_model(d, checkpoint_files[m], config_files[m],classes, color,m)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "a = cv2.imread('/content/drive/MyDrive/data/Train/Rural/masks_png/0.png')\n",
        "color =[[0,0,0],[255, 255, 255], [255, 0, 0], [255, 255, 0], [0, 0, 255],\n",
        "                 [159, 129, 183], [0, 255, 0], [255, 195, 128]]\n",
        "classes = ('null','background', 'building', 'road', 'water', 'barren', 'forest',\n",
        "                   'agricultural')\n",
        "seg = a[:,:,0]\n",
        "color_seg = np.zeros((seg.shape[0], seg.shape[1], 3), dtype=np.uint8)\n",
        "\n",
        "    # color_seg = color_seg[..., ::-1]\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(color_seg)\n",
        "patches = [m_patches.Patch(color=np.array(color[i]) / 256.,\n",
        "                                   label=classes[i]) for i in range(len(color))]\n",
        "        # put those patched as legend-handles into the legend\n",
        "plt.legend(handles=patches, bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.,\n",
        "            fontsize='large')\n",
        "plt.show()\n",
        "# Agent.show_result(cv2.imread('/content/drive/MyDrive/data/Train/Rural/images_png/0.png'),color_seg,color,classes,m)"
      ],
      "metadata": {
        "id": "7GpjTH4i4-4O"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "ImageSegmentation.ipynb",
      "provenance": [],
      "mount_file_id": "1oGPPKfYxZHdNHCe8_8x4rAqT91_r3ut-",
      "authorship_tag": "ABX9TyPGqHxkPhzijyi7Is3zsjAC",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}